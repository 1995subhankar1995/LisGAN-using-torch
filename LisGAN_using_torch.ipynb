{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LisGAN using torch.ipynb",
      "provenance": [],
      "mount_file_id": "15wyErbgb2Jdh32iEGxJRWbD1CaRr54AY",
      "authorship_tag": "ABX9TyOZ0BOCany0DQepssz59sTM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1995subhankar1995/LisGAN-using-torch/blob/master/LisGAN_using_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw7YONB_lFpZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b6d2c8fc-3e39-47f7-ed56-1b886160675a"
      },
      "source": [
        "!pip3 install https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-linux_x86_64.whl\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.0.0 from https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8neqS39knfqm",
        "colab_type": "text"
      },
      "source": [
        "# util.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_aJ8OypniO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import h5py\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import torch\n",
        "from sklearn import preprocessing\n",
        "import sys\n",
        "from sklearn.cluster import KMeans\n",
        "#val\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def map_label(label, classes):\n",
        "    mapped_label = torch.LongTensor(label.size())\n",
        "    for i in range(classes.size(0)):\n",
        "        mapped_label[label==classes[i]] = i    \n",
        "\n",
        "    return mapped_label\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "        f = open(self.filename+'.log', \"a\")\n",
        "        f.close()\n",
        "\n",
        "    def write(self, message):\n",
        "        f = open(self.filename+'.log', \"a\")\n",
        "        f.write(message)  \n",
        "        f.close()\n",
        "\n",
        "class DATA_LOADER(object):\n",
        "    def __init__(self, opt):\n",
        "        if opt.matdataset:\n",
        "            if opt.dataset == 'imageNet1K':\n",
        "                self.read_matimagenet(opt)\n",
        "            else:\n",
        "                self.read_matdataset(opt)\n",
        "        self.index_in_epoch = 0\n",
        "        self.epochs_completed = 0\n",
        "\n",
        "        self.feature_dim = self.train_feature.shape[1]\n",
        "        self.att_dim = self.attribute.shape[1]\n",
        "        self.text_dim = self.att_dim\n",
        "        self.train_cls_num = self.seenclasses.shape[0]\n",
        "        self.test_cls_num = self.unseenclasses.shape[0]\n",
        "        self.tr_cls_centroid = np.zeros([self.seenclasses.shape[0], self.feature_dim], np.float32)\n",
        "        for i in range(self.seenclasses.shape[0]):\n",
        "            self.tr_cls_centroid[i] = np.mean(self.train_feature[torch.nonzero(self.train_mapped_label == i),:].numpy(), axis=0)\n",
        "\n",
        "        n_cluster = opt.n_clusters\n",
        "        real_proto = torch.zeros(n_cluster * self.train_cls_num, self.feature_dim)\n",
        "        for i in range(self.train_cls_num):\n",
        "            sample_idx = (self.train_mapped_label == i).nonzero().squeeze()\n",
        "            if sample_idx.numel() == 0:\n",
        "                real_proto[n_cluster * i: n_cluster * (i+1)] = torch.zeros(n_cluster, self.feature_dim)\n",
        "            else:\n",
        "                real_sample_cls = self.train_feature[sample_idx, :]\n",
        "                y_pred = KMeans(n_clusters=n_cluster, random_state=3).fit_predict(real_sample_cls)\n",
        "                for j in range(n_cluster):\n",
        "                    real_proto[n_cluster*i+j] = torch.from_numpy(real_sample_cls[torch.nonzero(torch.from_numpy(y_pred)==j),:].mean(dim=0).cpu().numpy())\n",
        "        self.real_proto = real_proto\n",
        "\n",
        "                \n",
        "    # not tested\n",
        "    def read_h5dataset(self, opt):\n",
        "        # read image feature\n",
        "        fid = h5py.File(opt.dataroot + \"/\" + opt.dataset + \"/\" + opt.image_embedding + \".hdf5\", 'r')\n",
        "        feature = fid['feature'][()]\n",
        "        label = fid['label'][()] \n",
        "        trainval_loc = fid['trainval_loc'][()] \n",
        "        train_loc = fid['train_loc'][()] \n",
        "        val_unseen_loc = fid['val_unseen_loc'][()] \n",
        "        test_seen_loc = fid['test_seen_loc'][()] \n",
        "        test_unseen_loc = fid['test_unseen_loc'][()] \n",
        "        fid.close()\n",
        "        # read attributes\n",
        "        fid = h5py.File(opt.dataroot + \"/\" + opt.dataset + \"/\" + opt.class_embedding + \".hdf5\", 'r')\n",
        "        self.attribute = fid['attribute'][()]\n",
        "        fid.close()\n",
        "\n",
        "        if not opt.validation:\n",
        "            self.train_feature = feature[trainval_loc] \n",
        "            self.train_label = label[trainval_loc] \n",
        "            self.test_unseen_feature = feature[test_unseen_loc] \n",
        "            self.test_unseen_label = label[test_unseen_loc] \n",
        "            self.test_seen_feature = feature[test_seen_loc] \n",
        "            self.test_seen_label = label[test_seen_loc] \n",
        "        else:\n",
        "            self.train_feature = feature[train_loc] \n",
        "            self.train_label = label[train_loc] \n",
        "            self.test_unseen_feature = feature[val_unseen_loc] \n",
        "            self.test_unseen_label = label[val_unseen_loc] \n",
        "\n",
        "        self.seenclasses = np.unique(self.train_label)\n",
        "        self.unseenclasses = np.unique(self.test_unseen_label)\n",
        "        self.nclasses = self.seenclasses.size(0)\n",
        "\n",
        "    def read_matimagenet(self, opt):\n",
        "        if opt.preprocessing:\n",
        "            print('MinMaxScaler...')\n",
        "            scaler = preprocessing.MinMaxScaler()\n",
        "            matcontent = h5py.File(opt.dataroot + \"/\" + opt.dataset + \"/\" + opt.image_embedding + \".mat\", 'r')\n",
        "            feature = scaler.fit_transform(np.array(matcontent['features']))\n",
        "            label = np.array(matcontent['labels']).astype(int).squeeze() - 1\n",
        "            feature_val = scaler.transform(np.array(matcontent['features_val']))\n",
        "            label_val = np.array(matcontent['labels_val']).astype(int).squeeze() - 1\n",
        "            matcontent.close()\n",
        "            matcontent = h5py.File('/BS/xian/work/data/imageNet21K/extract_res/res101_1crop_2hops_t.mat', 'r')\n",
        "            feature_unseen = scaler.transform(np.array(matcontent['features']))\n",
        "            label_unseen = np.array(matcontent['labels']).astype(int).squeeze() - 1\n",
        "            matcontent.close()\n",
        "        else:\n",
        "            matcontent = h5py.File(opt.dataroot + \"/\" + opt.dataset + \"/\" + opt.image_embedding + \".mat\", 'r')\n",
        "            feature = np.array(matcontent['features'])\n",
        "            label = np.array(matcontent['labels']).astype(int).squeeze() - 1\n",
        "            feature_val = np.array(matcontent['features_val'])\n",
        "            label_val = np.array(matcontent['labels_val']).astype(int).squeeze() - 1\n",
        "            matcontent.close()\n",
        "\n",
        "\n",
        "        matcontent = sio.loadmat(opt.dataroot + \"/\" + opt.dataset + \"/\" + opt.class_embedding + \".mat\")\n",
        "        self.attribute = torch.from_numpy(matcontent['w2v']).float()\n",
        "        self.train_feature = torch.from_numpy(feature).float()\n",
        "        self.train_label = torch.from_numpy(label).long() \n",
        "        self.test_seen_feature = torch.from_numpy(feature_val).float()\n",
        "        self.test_seen_label = torch.from_numpy(label_val).long() \n",
        "        self.test_unseen_feature = torch.from_numpy(feature_unseen).float()\n",
        "        self.test_unseen_label = torch.from_numpy(label_unseen).long() \n",
        "        self.ntrain = self.train_feature.size()[0]\n",
        "        self.seenclasses = torch.from_numpy(np.unique(self.train_label.numpy()))\n",
        "        self.unseenclasses = torch.from_numpy(np.unique(self.test_unseen_label.numpy()))\n",
        "        self.train_class = torch.from_numpy(np.unique(self.train_label.numpy()))\n",
        "        self.ntrain_class = self.seenclasses.size(0)\n",
        "        self.ntest_class = self.unseenclasses.size(0)\n",
        "\n",
        "\n",
        "    def read_matdataset(self, opt):\n",
        "        #path = 'xlsa17/data/'\n",
        "        matcontent = sio.loadmat(opt.dataroot + \"/\" + opt.dataset + \"/\" + opt.image_embedding + \".mat\")\n",
        "        feature = matcontent['features'].T\n",
        "        label = matcontent['labels'].astype(int).squeeze() - 1\n",
        "        matcontent = sio.loadmat(opt.dataroot + \"/\" + opt.dataset + \"/\" + opt.class_embedding + \"_splits.mat\")\n",
        "        # numpy array index starts from 0, matlab starts from 1\n",
        "        trainval_loc = matcontent['trainval_loc'].squeeze() - 1\n",
        "        train_loc = matcontent['train_loc'].squeeze() - 1\n",
        "        val_unseen_loc = matcontent['val_loc'].squeeze() - 1\n",
        "        test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1\n",
        "        test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1\n",
        "\n",
        "        self.attribute = torch.from_numpy(matcontent['att'].T).float()\n",
        "        if not opt.validation:\n",
        "            if opt.preprocessing:\n",
        "                if opt.standardization:\n",
        "                    print('standardization...')\n",
        "                    scaler = preprocessing.StandardScaler()\n",
        "                else:\n",
        "                    scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "                _train_feature = scaler.fit_transform(feature[trainval_loc])\n",
        "                _test_seen_feature = scaler.transform(feature[test_seen_loc])\n",
        "                _test_unseen_feature = scaler.transform(feature[test_unseen_loc])\n",
        "                self.train_feature = torch.from_numpy(_train_feature).float()\n",
        "                mx = self.train_feature.max()\n",
        "                self.train_feature.mul_(1/mx)\n",
        "                self.train_label = torch.from_numpy(label[trainval_loc]).long()\n",
        "                self.test_unseen_feature = torch.from_numpy(_test_unseen_feature).float()\n",
        "                self.test_unseen_feature.mul_(1/mx)\n",
        "                self.test_unseen_label = torch.from_numpy(label[test_unseen_loc]).long()\n",
        "                self.test_seen_feature = torch.from_numpy(_test_seen_feature).float()\n",
        "                self.test_seen_feature.mul_(1/mx)\n",
        "                self.test_seen_label = torch.from_numpy(label[test_seen_loc]).long()\n",
        "            else:\n",
        "                self.train_feature = torch.from_numpy(feature[trainval_loc]).float()\n",
        "                self.train_label = torch.from_numpy(label[trainval_loc]).long()\n",
        "                self.test_unseen_feature = torch.from_numpy(feature[test_unseen_loc]).float()\n",
        "                self.test_unseen_label = torch.from_numpy(label[test_unseen_loc]).long()\n",
        "                self.test_seen_feature = torch.from_numpy(feature[test_seen_loc]).float()\n",
        "                self.test_seen_label = torch.from_numpy(label[test_seen_loc]).long()\n",
        "        else:\n",
        "            self.train_feature = torch.from_numpy(feature[train_loc]).float()\n",
        "            self.train_label = torch.from_numpy(label[train_loc]).long()\n",
        "            self.test_unseen_feature = torch.from_numpy(feature[val_unseen_loc]).float()\n",
        "            self.test_unseen_label = torch.from_numpy(label[val_unseen_loc]).long()\n",
        "\n",
        "        self.seenclasses = torch.from_numpy(np.unique(self.train_label.numpy()))\n",
        "        self.unseenclasses = torch.from_numpy(np.unique(self.test_unseen_label.numpy()))\n",
        "        self.ntrain = self.train_feature.size()[0]\n",
        "        self.ntrain_class = self.seenclasses.size(0)\n",
        "        self.ntest_class = self.unseenclasses.size(0)\n",
        "        self.train_class = self.seenclasses.clone()\n",
        "        self.allclasses = torch.arange(0, self.ntrain_class+self.ntest_class).long()\n",
        "        self.train_mapped_label = map_label(self.train_label, self.seenclasses)\n",
        "        self.train_att = self.attribute[self.seenclasses].numpy()\n",
        "        self.test_att  = self.attribute[self.unseenclasses].numpy()\n",
        "        self.train_cls_num = self.ntrain_class\n",
        "        self.test_cls_num  = self.ntest_class\n",
        "\n",
        "    def next_batch_one_class(self, batch_size):\n",
        "        if self.index_in_epoch == self.ntrain_class:\n",
        "            self.index_in_epoch = 0 \n",
        "            perm = torch.randperm(self.ntrain_class)\n",
        "            self.train_class[perm] = self.train_class[perm]\n",
        "\n",
        "        iclass = self.train_class[self.index_in_epoch]\n",
        "        idx = self.train_label.eq(iclass).nonzero().squeeze()\n",
        "        perm = torch.randperm(idx.size(0))\n",
        "        idx = idx[perm]\n",
        "        iclass_feature = self.train_feature[idx]\n",
        "        iclass_label = self.train_label[idx]\n",
        "        self.index_in_epoch += 1\n",
        "        return iclass_feature[0:batch_size], iclass_label[0:batch_size], self.attribute[iclass_label[0:batch_size]] \n",
        "    \n",
        "    def next_batch(self, batch_size):\n",
        "        idx = torch.randperm(self.ntrain)[0:batch_size]\n",
        "        batch_feature = self.train_feature[idx]\n",
        "        batch_label = self.train_label[idx]\n",
        "        batch_att = self.attribute[batch_label]\n",
        "        return batch_feature, batch_label, batch_att\n",
        "    #classifier\n",
        "    # select batch samples by randomly drawing batch_size classes    \n",
        "    def next_batch_uniform_class(self, batch_size):\n",
        "        batch_class = torch.LongTensor(batch_size)\n",
        "        for i in range(batch_size):\n",
        "            idx = torch.randperm(self.ntrain_class)[0]\n",
        "            batch_class[i] = self.train_class[idx]\n",
        "            \n",
        "        batch_feature = torch.FloatTensor(batch_size, self.train_feature.size(1))       \n",
        "        batch_label = torch.LongTensor(batch_size)\n",
        "        batch_att = torch.FloatTensor(batch_size, self.attribute.size(1))\n",
        "        for i in range(batch_size):\n",
        "            iclass = batch_class[i]\n",
        "            idx_iclass = self.train_label.eq(iclass).nonzero().squeeze()\n",
        "            idx_in_iclass = torch.randperm(idx_iclass.size(0))[0]\n",
        "            idx_file = idx_iclass[idx_in_iclass]\n",
        "            batch_feature[i] = self.train_feature[idx_file]\n",
        "            batch_label[i] = self.train_label[idx_file]\n",
        "            batch_att[i] = self.attribute[batch_label[i]] \n",
        "        return batch_feature, batch_label, batch_att"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x49_ZY4y7UN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LINEAR_LOGSOFTMAX(nn.Module):\n",
        "    def __init__(self, input_dim, nclass):\n",
        "        super(LINEAR_LOGSOFTMAX, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, nclass)\n",
        "        self.logic = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, x): \n",
        "        o = self.logic(self.fc(x))\n",
        "        return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNYiyzww9aQF",
        "colab_type": "text"
      },
      "source": [
        "# classifier.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0B_kRB61Ptz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "import sys\n",
        "\n",
        "class CLASSIFIER:\n",
        "    # train_Y is interger \n",
        "    def __init__(self, _train_X, _train_Y,  _nclass, _input_dim, _cuda, _lr=0.001, _beta1=0.5, _nepoch=20, _batch_size=100, pretrain_classifer=''):\n",
        "        self.train_X =  _train_X \n",
        "        self.train_Y = _train_Y \n",
        "        self.batch_size = _batch_size\n",
        "        self.nepoch = _nepoch\n",
        "        self.nclass = _nclass\n",
        "        self.dim = _input_dim\n",
        "        self.cuda = _cuda\n",
        "        self.model =  LINEAR_LOGSOFTMAX(self.dim, self.nclass)\n",
        "        self.model.apply(weights_init)\n",
        "        self.criterion = nn.NLLLoss()\n",
        "        \n",
        "        self.input = torch.FloatTensor(_batch_size, self.dim) \n",
        "        self.label = torch.LongTensor(_batch_size) \n",
        "        \n",
        "        self.lr = _lr\n",
        "        self.beta1 = _beta1\n",
        "        # setup optimizer\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=_lr, betas=(_beta1, 0.999))\n",
        "\n",
        "        if self.cuda:\n",
        "            self.model.cuda()\n",
        "            self.criterion.cuda()\n",
        "            self.input = self.input.cuda()\n",
        "            self.label = self.label.cuda()\n",
        "\n",
        "        self.index_in_epoch = 0\n",
        "        self.epochs_completed = 0\n",
        "        self.ntrain = self.train_X.size()[0]\n",
        "        #fake\n",
        "        if pretrain_classifer == '':\n",
        "            self.fit()\n",
        "        else:\n",
        "            self.model.load_state_dict(torch.load(pretrain_classifier))\n",
        "    \n",
        "\n",
        "    def fit(self):\n",
        "        for epoch in range(self.nepoch):\n",
        "            for i in range(0, self.ntrain, self.batch_size):      \n",
        "                self.model.zero_grad()\n",
        "                batch_input, batch_label = self.next_batch(self.batch_size) \n",
        "                self.input.copy_(batch_input)\n",
        "                self.label.copy_(batch_label)\n",
        "                   \n",
        "                inputv = Variable(self.input)\n",
        "                labelv = Variable(self.label)\n",
        "                output = self.model(inputv)\n",
        "                loss = self.criterion(output, labelv)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                     \n",
        "    def next_batch(self, batch_size):\n",
        "        start = self.index_in_epoch\n",
        "        # shuffle the data at the first epoch\n",
        "        if self.epochs_completed == 0 and start == 0:\n",
        "            perm = torch.randperm(self.ntrain)\n",
        "            self.train_X = self.train_X[perm]\n",
        "            self.train_Y = self.train_Y[perm]\n",
        "        # the last batch\n",
        "        if start + batch_size > self.ntrain:\n",
        "            self.epochs_completed += 1\n",
        "            rest_num_examples = self.ntrain - start\n",
        "            if rest_num_examples > 0:\n",
        "                X_rest_part = self.train_X[start:self.ntrain]\n",
        "                Y_rest_part = self.train_Y[start:self.ntrain]\n",
        "            # shuffle the data\n",
        "            perm = torch.randperm(self.ntrain)\n",
        "            self.train_X = self.train_X[perm]\n",
        "            self.train_Y = self.train_Y[perm]\n",
        "            # start next epoch\n",
        "            start = 0\n",
        "            self.index_in_epoch = batch_size - rest_num_examples\n",
        "            end = self.index_in_epoch\n",
        "            X_new_part = self.train_X[start:end]\n",
        "            Y_new_part = self.train_Y[start:end]\n",
        "            if rest_num_examples > 0:\n",
        "                return torch.cat((X_rest_part, X_new_part), 0) , torch.cat((Y_rest_part, Y_new_part), 0)\n",
        "            else:\n",
        "                return X_new_part, Y_new_part\n",
        "        else:\n",
        "            self.index_in_epoch += batch_size\n",
        "            end = self.index_in_epoch\n",
        "            # from index start to index end-1\n",
        "            return self.train_X[start:end], self.train_Y[start:end]\n",
        "\n",
        "    def compute_per_class_acc(self, test_label, predicted_label, nclass):\n",
        "        acc_per_class = torch.FloatTensor(nclass).fill_(0)\n",
        "        for i in range(nclass):\n",
        "            idx = (test_label == i)\n",
        "            acc_per_class[i] = torch.sum(test_label[idx]==predicted_label[idx]).float()/ torch.sum(idx)\n",
        "        return acc_per_class.mean() \n",
        "\n",
        "    # test_label is integer \n",
        "    def val(self, test_X, test_label, target_classes): \n",
        "        start = 0\n",
        "        ntest = test_X.size()[0]\n",
        "        predicted_label = torch.LongTensor(test_label.size())\n",
        "        for i in range(0, ntest, self.batch_size):\n",
        "            end = min(ntest, start+self.batch_size)\n",
        "            if self.cuda:\n",
        "                output = self.model(Variable(test_X[start:end].cuda(), volatile=True)) \n",
        "            else:\n",
        "                output = self.model(Variable(test_X[start:end], volatile=True)) \n",
        "            _, predicted_label[start:end] = torch.max(output.data, 1)\n",
        "            start = end\n",
        "\n",
        "        acc = self.compute_per_class_acc(map_label(test_label, target_classes), predicted_label, target_classes.size(0))\n",
        "        return acc\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdFiQG4oGLuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-aEWd7dm9XK",
        "colab_type": "text"
      },
      "source": [
        "# classifier2.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-VxQiCDmp4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from termcolor import cprint\n",
        "\n",
        "\n",
        "import itertools\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from scipy.stats import entropy\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import time\n",
        "\n",
        "#CLASSIFIER\n",
        "class CLASSIFIER2:\n",
        "    # train_Y is interger\n",
        "    # CLASSIFIER(syn_feature,util.map_label(syn_label,data.unseenclasses),data,data.unseenclasses.size(0),opt.cuda,opt.classifier_lr, 0.5, 25, opt.syn_num, False)\n",
        "    def __init__(self, _train_X, _train_Y, data_loader, _nclass, _cuda, _lr=0.001, _beta1=0.5, _nepoch=20, _batch_size=100, generalized=True,ratio=0.6,epoch=20):\n",
        "        self.train_X =  _train_X \n",
        "        self.train_Y = _train_Y \n",
        "        self.test_seen_feature = data_loader.test_seen_feature\n",
        "        self.test_seen_label = data_loader.test_seen_label \n",
        "        self.test_unseen_feature = data_loader.test_unseen_feature\n",
        "        self.test_unseen_label = data_loader.test_unseen_label \n",
        "        self.seenclasses = data_loader.seenclasses\n",
        "        self.unseenclasses = data_loader.unseenclasses\n",
        "        self.batch_size = _batch_size\n",
        "        self.nepoch = _nepoch\n",
        "        self.nclass = _nclass\n",
        "        self.input_dim = _train_X.size(1)\n",
        "        self.cuda = _cuda\n",
        "        self.model =  LINEAR_LOGSOFTMAX(self.input_dim, self.nclass)\n",
        "        self.model.apply(weights_init)\n",
        "        self.criterion = nn.NLLLoss()\n",
        "\n",
        "        self.data = data_loader\n",
        "\n",
        "        self.input = torch.FloatTensor(_batch_size, self.input_dim) \n",
        "        self.label = torch.LongTensor(_batch_size) \n",
        "        \n",
        "        self.lr = _lr\n",
        "        self.beta1 = _beta1\n",
        "        # setup optimizer\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=_lr, betas=(_beta1, 0.999))\n",
        "        self.ratio = ratio\n",
        "        self.epoch = epoch\n",
        "\n",
        "        if self.cuda:\n",
        "            self.model.cuda()\n",
        "            self.criterion.cuda()\n",
        "            self.input = self.input.cuda()\n",
        "            self.label = self.label.cuda()\n",
        "\n",
        "        self.index_in_epoch = 0\n",
        "        self.epochs_completed = 0\n",
        "        self.ntrain = self.train_X.size()[0]\n",
        "        self.backup_X = _train_X\n",
        "        self.backup_Y = _train_Y\n",
        "\n",
        "        if generalized:\n",
        "            self.acc_seen, self.acc_unseen, self.H = self.fit()\n",
        "        else:\n",
        "            self.acc = self.fit_zsl()\n",
        "\n",
        "    def pairwise_distances(self,x, y=None):\n",
        "        '''\n",
        "        Input: x is a Nxd matrix\n",
        "               y is an optional Mxd matirx\n",
        "        Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n",
        "                if y is not given then use 'y=x'.\n",
        "        i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n",
        "        '''\n",
        "        x_norm = (x ** 2).sum(1).view(-1, 1)\n",
        "        if y is not None:\n",
        "            y_t = torch.transpose(y, 0, 1)\n",
        "            y_norm = (y ** 2).sum(1).view(1, -1)\n",
        "        else:\n",
        "            y_t = torch.transpose(x, 0, 1)\n",
        "            y_norm = x_norm.view(1, -1)\n",
        "\n",
        "        dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
        "        # Ensure diagonal is zero if x=y\n",
        "        if y is None:\n",
        "            dist = dist - torch.diag(dist.diag)\n",
        "        return torch.clamp(dist, 0.0, np.inf)\n",
        "\n",
        "    def fit_zsl(self):\n",
        "        first_acc=0\n",
        "        first_all_pred = None\n",
        "        first_all_output = None\n",
        "        first_all_acc = []\n",
        "\n",
        "        trun = lambda x: int(x * 100)\n",
        "\n",
        "        all_length = self.test_unseen_feature.size(0)\n",
        "        mapped_test_label = map_label(self.test_unseen_label, self.unseenclasses)\n",
        "        for epoch in range(self.nepoch):\n",
        "            for i in range(0, self.ntrain, self.batch_size):\n",
        "                self.model.zero_grad()\n",
        "                batch_input, batch_label = self.next_batch(self.batch_size)\n",
        "                self.input.copy_(batch_input)\n",
        "                self.label.copy_(batch_label)\n",
        "                inputv = Variable(self.input)  # fake_feature\n",
        "                labelv = Variable(self.label)  # fake_labels\n",
        "                output = self.model(inputv)\n",
        "                loss = self.criterion(output, labelv)  # fake_unseen_feature labels\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            acc, pred, output,all_acc = self.val(self.test_unseen_feature, self.test_unseen_label, self.unseenclasses)\n",
        "            if acc > first_acc:\n",
        "                first_acc = acc\n",
        "                first_all_pred = pred\n",
        "                first_all_output = output\n",
        "                first_all_acc = all_acc\n",
        "        print('First Acc: {:.2f}%'.format(first_acc * 100))\n",
        "        # print([trun(x) for  x in  list(first_all_acc)])\n",
        "        # cprint('First Acc: {:.2f}%'.format(first_acc * 100),'red')\n",
        "        easy_len = int(all_length*self.ratio)\n",
        "        hard_len = all_length - easy_len\n",
        "        entropy_value = torch.from_numpy(np.asarray(list(map(entropy,first_all_output.data))))\n",
        "        _, indices = torch.sort(-entropy_value)\n",
        "        exit_indices = indices[:easy_len]\n",
        "        keep_indices = indices[easy_len:]\n",
        "\n",
        "        first_easy_pred = first_all_pred[exit_indices]\n",
        "        first_easy_label = mapped_test_label[exit_indices]\n",
        "        first_hard_pred = first_all_pred[keep_indices]\n",
        "        first_hard_label = mapped_test_label[keep_indices]\n",
        "        acc_first_easy = self.compute_per_class_acc(first_easy_label,first_easy_pred,self.unseenclasses.size(0))\n",
        "        acc_first_hard = self.compute_per_class_acc(first_hard_label,first_hard_pred,self.unseenclasses.size(0))\n",
        "        all_easy_hard_label = torch.cat( (first_easy_label,first_hard_label),0 )\n",
        "        # print('First Easy Acc:{:.2f}%'.format(acc_first_easy*100))\n",
        "        # print('First Hard Acc:{:.2f}%'.format(acc_first_hard*100))\n",
        "\n",
        "        self.index_in_epoch = 0\n",
        "        self.epochs_completed = 0\n",
        "        self.ntrain = self.backup_X.size()[0] + easy_len # fake+easy_sample\n",
        "        self.train_X = torch.cat( (self.backup_X, self.test_unseen_feature[exit_indices] ),0 )\n",
        "        self.train_Y = torch.cat( (self.backup_Y, first_easy_pred ),0 )\n",
        "\n",
        "        sims = self.pairwise_distances(self.test_unseen_feature[keep_indices], self.train_X)\n",
        "        value,idx = torch.min(sims,dim=1)\n",
        "        knn_hard_pred = self.train_Y[idx]\n",
        "        knn_all_pred = torch.cat( (first_easy_pred,knn_hard_pred),0 )\n",
        "\n",
        "        acc_knn_hard = self.compute_per_class_acc(first_hard_label, knn_hard_pred,self.unseenclasses.size(0))\n",
        "        acc_knn = self.compute_per_class_acc(all_easy_hard_label,knn_all_pred,self.unseenclasses.size(0))\n",
        "        all_acc_knn = self.compute_every_class_acc(all_easy_hard_label,knn_all_pred,self.unseenclasses.size(0))\n",
        "        # print('1NN Hard Acc: {:.2f}%'.format(acc_knn_hard*100))\n",
        "        print('1NN   Acc: {:.2f}%'.format(acc_knn*100))\n",
        "        # print([trun(x) for x in list(all_acc_knn)])\n",
        "\n",
        "        acc_fc_hard = 0\n",
        "        fc_hard_pred = None\n",
        "        real_mean_acc = 0\n",
        "        for epoch in range(self.nepoch):\n",
        "            for i in range(0, self.ntrain, self.batch_size):\n",
        "                self.model.zero_grad()\n",
        "                batch_input, batch_label = self.next_batch(self.batch_size)\n",
        "                self.input.copy_(batch_input)\n",
        "                self.label.copy_(batch_label)\n",
        "\n",
        "                inputv = Variable(self.input)  # fake_feature\n",
        "                labelv = Variable(self.label)  # fake_labels\n",
        "                output = self.model(inputv)\n",
        "                loss = self.criterion(output, labelv)  # 使用fake_unseen_feature和labels来训练分类器\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "            acc, pred, output,_ = self.val(self.test_unseen_feature[keep_indices],self.test_unseen_label[keep_indices],self.unseenclasses)\n",
        "            correct_num = pred.eq(mapped_test_label[keep_indices]).cpu().sum().float()\n",
        "            all_length = output.size(0)\n",
        "            first_acc = correct_num / all_length\n",
        "            if first_acc > acc_fc_hard:\n",
        "                acc_fc_hard = acc\n",
        "                fc_hard_pred = pred\n",
        "                real_mean_acc = first_acc\n",
        "        fc_all_pred = torch.cat( (first_easy_pred,fc_hard_pred),0 )\n",
        "        acc_fc_hard = self.compute_per_class_acc(first_hard_label, fc_hard_pred,self.unseenclasses.size(0))\n",
        "        acc_fc = self.compute_per_class_acc(all_easy_hard_label,fc_all_pred,self.unseenclasses.size(0))\n",
        "        all_acc_fc = self.compute_every_class_acc(all_easy_hard_label,fc_all_pred,self.unseenclasses.size(0))\n",
        "        # print('FC Hard Acc: {:.2f}%'.format(acc_fc_hard*100))\n",
        "        print('FC    Acc: {:.2f}%'.format(acc_fc*100))\n",
        "        # print([trun(x) for x in list(all_acc_fc)])\n",
        "        # cprint('FC Overall Acc: {:.2f}%\\n'.format(acc_fc*100),'red')\n",
        "\n",
        "        sys.stdout.flush()\n",
        "        return acc_fc\n",
        "\n",
        "\n",
        "\n",
        "    def split_pred(self,all_pred, real_label):\n",
        "        seen_pred = None\n",
        "        seen_label = None\n",
        "        unseen_pred = None\n",
        "        unseen_label = None\n",
        "        for i in self.seenclasses:\n",
        "            idx = (real_label == i)\n",
        "            if seen_pred is None:\n",
        "                seen_pred = all_pred[idx]\n",
        "                seen_label = real_label[idx]\n",
        "            else:\n",
        "                seen_pred = torch.cat( (seen_pred,all_pred[idx]),0 )\n",
        "                seen_label = torch.cat( (seen_label, real_label[idx]) )\n",
        "\n",
        "        for i in self.unseenclasses:\n",
        "            idx = (real_label == i)\n",
        "            if unseen_pred is None:\n",
        "                unseen_pred = all_pred[idx]\n",
        "                unseen_label = real_label[idx]\n",
        "            else:\n",
        "                unseen_pred = torch.cat( (unseen_pred,all_pred[idx]),0 )\n",
        "                unseen_label = torch.cat(  (unseen_label, real_label[idx]), 0 )\n",
        "\n",
        "        return seen_pred, seen_label, unseen_pred, unseen_label\n",
        "\n",
        "\n",
        "\n",
        "    # for gzsl\n",
        "    def fit(self):\n",
        "        # 3个length\n",
        "        test_seen_length = self.test_seen_feature.shape[0] #1764\n",
        "        test_unseen_length = self.test_unseen_feature.shape[0] #2967\n",
        "        all_length = test_seen_length + test_unseen_length\n",
        "        all_test_feature = torch.cat( (self.test_seen_feature,self.test_unseen_feature), 0 )\n",
        "        all_test_label = torch.cat( (self.test_seen_label, self.test_unseen_label), 0 )\n",
        "        all_classes = torch.sort(torch.cat( (self.seenclasses,self.unseenclasses),0 ))[0]\n",
        "        first_acc = 0\n",
        "        first_all_pred = None\n",
        "        first_all_output = None\n",
        "\n",
        "        best_H = 0\n",
        "        for epoch in range(self.nepoch):\n",
        "            for i in range(0, self.ntrain, self.batch_size): #self.ntrain=22057, self.batch_size=300\n",
        "                self.model.zero_grad()\n",
        "                batch_input, batch_label = self.next_batch(self.batch_size)\n",
        "                self.input.copy_(batch_input)\n",
        "                self.label.copy_(batch_label)\n",
        "\n",
        "                inputv = Variable(self.input)\n",
        "                labelv = Variable(self.label)\n",
        "                output = self.model(inputv)\n",
        "                loss = self.criterion(output, labelv)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            acc_seen,pred_seen,output_seen = self.val_gzsl(self.test_seen_feature, self.test_seen_label, self.seenclasses)\n",
        "            acc_unseen,pred_unseen,output_unseen = self.val_gzsl(self.test_unseen_feature, self.test_unseen_label, self.unseenclasses)\n",
        "            H = 2 * acc_seen * acc_unseen / (acc_seen + acc_unseen)\n",
        "            if H > best_H:\n",
        "                best_H = H\n",
        "                first_all_pred = torch.cat( (pred_seen,pred_unseen), 0 )\n",
        "                first_all_output = torch.cat( (output_seen, output_unseen), 0 )\n",
        "\n",
        "        first_seen_pred,first_seen_label,first_unseen_pred,first_unseen_label = self.split_pred(first_all_pred,all_test_label)\n",
        "        #def compute_per_class_acc_gzsl(self, test_label, predicted_label, target_classes):\n",
        "        acc_first_seen = self.compute_per_class_acc_gzsl(first_seen_label, first_seen_pred,self.seenclasses)\n",
        "        acc_first_unseen = self.compute_per_class_acc_gzsl(first_unseen_label, first_unseen_pred,self.unseenclasses)\n",
        "        acc_first_H = 2*acc_first_seen*acc_first_unseen/(acc_first_seen+acc_first_unseen)\n",
        "        print('First Seen: {:.2f}%, Unseen: {:.2f}%, First H: {:.2f}%'.format(acc_first_seen*100,acc_first_unseen*100,acc_first_H*100))\n",
        "        # print('First Unseen Acc: {:.2f}%'.format(acc_first_unseen*100))\n",
        "        # print('First Harmonic Acc: {:.2f}%\\n'.format(acc_first_H*100))\n",
        "\n",
        "        easy_length = int(all_length*self.ratio)\n",
        "        hard_length = all_length - easy_length\n",
        "        entropy_value = torch.from_numpy(np.asarray(list(map(entropy, first_all_output.data))))\n",
        "        _, indices = torch.sort(-entropy_value)\n",
        "        exit_indices = indices[:easy_length]\n",
        "        keep_indices = indices[easy_length:]\n",
        "        first_easy_pred = first_all_pred[exit_indices]\n",
        "        first_easy_label = all_test_label[exit_indices]\n",
        "        first_hard_pred = first_all_pred[keep_indices]\n",
        "        first_hard_label = all_test_label[keep_indices]\n",
        "        all_easy_hard_label = torch.cat( (first_easy_label,first_hard_label),0 )\n",
        "\n",
        "        acc_first_easy = self.compute_per_class_acc_gzsl(first_easy_label,first_easy_pred,all_classes)\n",
        "        acc_first_hard = self.compute_per_class_acc_gzsl(first_hard_label,first_hard_pred,all_classes)\n",
        "        # print('First Easy Acc: {:.2f}%'.format(acc_first_easy*100))\n",
        "        # print('First Hard Acc: {:.2f}%'.format(acc_first_hard*100))\n",
        "\n",
        "        self.index_in_epoch = 0\n",
        "        self.epochs_completed = 0\n",
        "        self.ntrain = self.backup_X.size(0) + easy_length\n",
        "        self.train_X = torch.cat( (self.backup_X, all_test_feature[exit_indices]),0 )\n",
        "        self.train_Y = torch.cat( (self.backup_Y, first_easy_pred),0)\n",
        "\n",
        "        sims = self.pairwise_distances(all_test_feature[keep_indices], self.train_X)\n",
        "        value, idx = torch.min(sims, dim=1)\n",
        "        knn_hard_pred = self.train_Y[idx]\n",
        "        knn_all_pred = torch.cat( (first_easy_pred,knn_hard_pred),0 )\n",
        "        knn_seen_pred,knn_seen_label,knn_unseen_pred,knn_unseen_label = self.split_pred(knn_all_pred,all_easy_hard_label)\n",
        "        acc_knn_seen = self.compute_per_class_acc_gzsl(knn_seen_label,knn_seen_pred,self.seenclasses)\n",
        "        acc_knn_unseen = self.compute_per_class_acc_gzsl(knn_unseen_label,knn_unseen_pred,self.unseenclasses)\n",
        "        acc_knn_H = 2*acc_knn_seen*acc_knn_unseen/(acc_knn_seen+acc_knn_unseen)\n",
        "        print('1NN   Seen: {:.2f}%, Unseen: {:.2f}%, 1NN H: {:.2f}%'.format(acc_knn_seen*100,acc_knn_unseen*100,acc_knn_H*100))\n",
        "        # print('1NN Unseen Acc: {:.2f}%'.format(acc_knn_unseen*100))\n",
        "        # print('1NN H Acc: {:.2f}%'.format(acc_knn_H*100))\n",
        "\n",
        "        # acc_knn_hard = self.compute_per_class_acc_gzsl(first_hard_label,knn_hard_pred,all_classes)\n",
        "        # print('1NN Hard Acc: {:.2f}%'.format(acc_knn_hard*100))\n",
        "\n",
        "        best_fc_hard_acc = 0\n",
        "        fc_hard_pred = None\n",
        "        for epoch in range(self.nepoch):\n",
        "            for i in range(0, self.ntrain, self.batch_size):\n",
        "                self.model.zero_grad()\n",
        "                batch_input, batch_label = self.next_batch(self.batch_size)\n",
        "                self.input.copy_(batch_input)\n",
        "                self.label.copy_(batch_label)\n",
        "                inputv = Variable(self.input)  # fake_feature\n",
        "                labelv = Variable(self.label)  # fake_labels\n",
        "                output = self.model(inputv)\n",
        "                loss = self.criterion(output, labelv)  # 使用fake_unseen_feature和labels来训练分类器\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "            acc,pred,_ = self.val_gzsl(all_test_feature[keep_indices],first_hard_label,all_classes)\n",
        "            if acc > best_fc_hard_acc:\n",
        "                fc_hard_pred = pred\n",
        "\n",
        "        fc_all_pred = torch.cat((first_easy_pred, fc_hard_pred), 0)\n",
        "        fc_seen_pred, fc_seen_label, fc_unseen_pred, fc_unseen_label = self.split_pred(fc_all_pred,all_easy_hard_label)\n",
        "        acc_fc_seen = self.compute_per_class_acc_gzsl(fc_seen_label, fc_seen_pred, self.seenclasses)\n",
        "        acc_fc_unseen = self.compute_per_class_acc_gzsl(fc_unseen_label, fc_unseen_pred, self.unseenclasses)\n",
        "        acc_fc_H = 2 * acc_fc_seen * acc_fc_unseen / (acc_fc_seen + acc_fc_unseen)\n",
        "        print('FC    Seen: {:.2f}%, Unseen: {:.2f}%, FC H: {:.2f}%'.format(acc_fc_seen * 100,acc_fc_unseen * 100,acc_fc_H * 100))\n",
        "        # print('FC Unseen Acc: {:.2f}%'.format(acc_fc_unseen * 100))\n",
        "        # print('FC H Acc: {:.2f}%'.format(acc_fc_H * 100))\n",
        "\n",
        "        acc_fc_hard = self.compute_per_class_acc_gzsl(first_hard_label,fc_hard_pred,all_classes)\n",
        "        # print('FC Hard Acc: {:.2f}%\\n'.format(acc_fc_hard*100))\n",
        "\n",
        "        sys.stdout.flush()\n",
        "        return acc_fc_seen,acc_fc_unseen,acc_fc_H\n",
        "\n",
        "    def val(self, test_X, test_label, target_classes,second=False):\n",
        "        start = 0\n",
        "        ntest = test_X.size()[0]\n",
        "        predicted_label = torch.LongTensor(test_label.size())\n",
        "        all_output = None\n",
        "        for i in range(0, ntest, self.batch_size):\n",
        "            end = min(ntest, start+self.batch_size)\n",
        "            if self.cuda:\n",
        "                output = self.model(Variable(test_X[start:end].cuda(), volatile=True))\n",
        "            else:\n",
        "                output = self.model(Variable(test_X[start:end], volatile=True))\n",
        "            if all_output is None:\n",
        "                all_output = output\n",
        "            else:\n",
        "                all_output = torch.cat( (all_output, output), 0 )\n",
        "            _, predicted_label[start:end] = torch.max(output.data, 1)\n",
        "            start = end\n",
        "        acc = self.compute_per_class_acc(map_label(test_label, target_classes), predicted_label, target_classes.size(0))\n",
        "        acc_all = self.compute_every_class_acc(map_label(test_label, target_classes), predicted_label, target_classes.size(0))\n",
        "        return acc, predicted_label, all_output,acc_all\n",
        "\n",
        "    def val_gzsl(self, test_X, test_label, target_classes):\n",
        "        start = 0\n",
        "        ntest = test_X.size()[0]\n",
        "        predicted_label = torch.LongTensor(test_label.size())\n",
        "        all_output = None\n",
        "        for i in range(0, ntest, self.batch_size):\n",
        "            end = min(ntest, start+self.batch_size)\n",
        "            if self.cuda:\n",
        "                output = self.model(Variable(test_X[start:end].cuda(), volatile=True))\n",
        "            else:\n",
        "                output = self.model(Variable(test_X[start:end], volatile=True))\n",
        "\n",
        "            if all_output is None:\n",
        "                all_output = output\n",
        "            else:\n",
        "                all_output = torch.cat( (all_output, output), 0 )\n",
        "            _, predicted_label[start:end] = torch.max(output.data, 1)\n",
        "            start = end\n",
        "        # acc = self.compute_per_class_acc(util.map_label(test_label, target_classes), predicted_label, target_classes.size(0))\n",
        "        acc = self.compute_per_class_acc_gzsl(test_label, predicted_label, target_classes)\n",
        "        return acc, predicted_label, all_output\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        start = self.index_in_epoch\n",
        "        # shuffle the data at the first epoch\n",
        "        if self.epochs_completed == 0 and start == 0:\n",
        "            perm = torch.randperm(self.ntrain)\n",
        "            self.train_X = self.train_X[perm]\n",
        "            self.train_Y = self.train_Y[perm]\n",
        "        # the last batch\n",
        "        if start + batch_size > self.ntrain:\n",
        "            self.epochs_completed += 1\n",
        "            rest_num_examples = self.ntrain - start\n",
        "            if rest_num_examples > 0:\n",
        "                X_rest_part = self.train_X[start:self.ntrain]\n",
        "                Y_rest_part = self.train_Y[start:self.ntrain]\n",
        "            # shuffle the data\n",
        "            perm = torch.randperm(self.ntrain)\n",
        "            self.train_X = self.train_X[perm]\n",
        "            self.train_Y = self.train_Y[perm]\n",
        "            # start next epoch\n",
        "            start = 0\n",
        "            self.index_in_epoch = batch_size - rest_num_examples\n",
        "            end = self.index_in_epoch\n",
        "            X_new_part = self.train_X[start:end]\n",
        "            Y_new_part = self.train_Y[start:end]\n",
        "            if rest_num_examples > 0:\n",
        "                return torch.cat((X_rest_part, X_new_part), 0) , torch.cat((Y_rest_part, Y_new_part), 0)\n",
        "            else:\n",
        "                return X_new_part, Y_new_part\n",
        "        else:\n",
        "            self.index_in_epoch += batch_size\n",
        "            end = self.index_in_epoch\n",
        "            return self.train_X[start:end], self.train_Y[start:end]\n",
        "\n",
        "    def compute_per_class_acc_gzsl(self, test_label, predicted_label, target_classes):\n",
        "        acc_per_class = 0\n",
        "        for i in target_classes:\n",
        "            idx = (test_label == i)\n",
        "            if torch.sum(idx) == 0:\n",
        "                continue\n",
        "            else:\n",
        "                acc_per_class += torch.sum(test_label[idx]==predicted_label[idx]) / torch.sum(idx)\n",
        "        acc_per_class /= target_classes.size(0)\n",
        "        return acc_per_class \n",
        "\n",
        "    def compute_per_class_acc(self, test_label, predicted_label, nclass):\n",
        "        acc_per_class = torch.FloatTensor(nclass).fill_(0)\n",
        "        for i in range(nclass):\n",
        "            idx = (test_label == i)\n",
        "            if torch.sum(idx) != 0:\n",
        "                acc_per_class[i] = torch.sum(test_label[idx]==predicted_label[idx]).float() / torch.sum(idx)\n",
        "        return acc_per_class.mean()\n",
        "\n",
        "    def compute_every_class_acc(self, test_label, predicted_label, nclass):\n",
        "        acc_per_class = torch.FloatTensor(nclass).fill_(0)\n",
        "        for i in range(nclass):\n",
        "            idx = (test_label == i)\n",
        "            if torch.sum(idx) != 0:\n",
        "                acc_per_class[i] = torch.sum(test_label[idx]==predicted_label[idx]).float() / torch.sum(idx)\n",
        "        return acc_per_class\n",
        "#util\n",
        "class LINEAR_LOGSOFTMAX(nn.Module):\n",
        "    def __init__(self, input_dim, nclass):\n",
        "        super(LINEAR_LOGSOFTMAX, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, nclass)\n",
        "        self.logic = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, x): \n",
        "        o = self.logic(self.fc(x))\n",
        "        return o  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqE2U79MnTYz",
        "colab_type": "text"
      },
      "source": [
        "models.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzhb42YZnVRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "#MLP_G\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "class MLP_AC_D(nn.Module):\n",
        "    def __init__(self, opt): \n",
        "        super(MLP_AC_D, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.resSize, opt.ndh)\n",
        "        self.disc_linear = nn.Linear(opt.ndh, 1)\n",
        "        self.aux_linear = nn.Linear(opt.ndh, opt.attSize)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.lrelu(self.fc1(x))\n",
        "        s = self.sigmoid(self.disc_linear(h))\n",
        "        a = self.aux_linear(h)\n",
        "        return s,a \n",
        "\n",
        "class MLP_AC_2HL_D(nn.Module):\n",
        "    def __init__(self, opt): \n",
        "        super(MLP_AC_2HL_D, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.resSize, opt.ndh)\n",
        "        self.fc2 = nn.Linear(opt.ndh, opt.ndh)\n",
        "        self.disc_linear = nn.Linear(opt.ndh, 1)\n",
        "        self.aux_linear = nn.Linear(opt.ndh, opt.attSize)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.dropout(self.lrelu(self.fc1(x)))\n",
        "        h = self.dropout(self.lrelu(self.fc2(h)))\n",
        "        s = self.sigmoid(self.disc_linear(h))\n",
        "        a = self.aux_linear(h)\n",
        "        return s,a \n",
        "\n",
        "class MLP_3HL_CRITIC(nn.Module):\n",
        "    def __init__(self, opt): \n",
        "        super(MLP_3HL_CRITIC, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.resSize + opt.attSize, opt.ndh)\n",
        "        self.fc2 = nn.Linear(opt.ndh, opt.ndh)\n",
        "        self.fc3 = nn.Linear(opt.ndh, opt.ndh)\n",
        "        self.fc4 = nn.Linear(opt.ndh, 1)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x, att):\n",
        "        h = torch.cat((x, att), 1) \n",
        "        h = self.lrelu(self.fc1(h))\n",
        "        h = self.lrelu(self.fc2(h))\n",
        "        h = self.lrelu(self.fc3(h))\n",
        "        h = self.fc4(h)\n",
        "        return h\n",
        "\n",
        "class MLP_2HL_CRITIC(nn.Module):\n",
        "    def __init__(self, opt): \n",
        "        super(MLP_2HL_CRITIC, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.resSize + opt.attSize, opt.ndh)\n",
        "        self.fc2 = nn.Linear(opt.ndh, opt.ndh)\n",
        "        self.fc3 = nn.Linear(opt.ndh, 1)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x, att):\n",
        "        h = torch.cat((x, att), 1) \n",
        "        h = self.lrelu(self.fc1(h))\n",
        "        h = self.lrelu(self.fc2(h))\n",
        "        h = self.fc3(h)\n",
        "        return h\n",
        "\n",
        "class MLP_2HL_Dropout_CRITIC(nn.Module):\n",
        "    def __init__(self, opt): \n",
        "        super(MLP_2HL_Dropout_CRITIC, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.resSize + opt.attSize, opt.ndh)\n",
        "        self.fc2 = nn.Linear(opt.ndh, opt.ndh)\n",
        "        self.fc3 = nn.Linear(opt.ndh, 1)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x, att):\n",
        "        h = torch.cat((x, att), 1) \n",
        "        h = self.dropout(self.lrelu(self.fc1(h)))\n",
        "        h = self.dropout(self.lrelu(self.fc2(h)))\n",
        "        h = self.fc3(h)\n",
        "        return h\n",
        "\n",
        "class MLP_CRITIC(nn.Module):\n",
        "    def __init__(self, opt): \n",
        "        super(MLP_CRITIC, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.resSize + opt.attSize, opt.ndh)\n",
        "        #self.fc2 = nn.Linear(opt.ndh, opt.ndh)\n",
        "        self.fc2 = nn.Linear(opt.ndh, 1)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x, att):\n",
        "        h = torch.cat((x, att), 1) \n",
        "        h = self.lrelu(self.fc1(h))\n",
        "        h = self.fc2(h)\n",
        "        return h\n",
        "\n",
        "class MLP_D(nn.Module):\n",
        "    def __init__(self, opt): \n",
        "        super(MLP_D, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.resSize + opt.attSize, opt.ndh)\n",
        "        self.fc2 = nn.Linear(opt.ndh, 1)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x, att):\n",
        "        h = torch.cat((x, att), 1) \n",
        "        h = self.lrelu(self.fc1(h))\n",
        "        h = self.sigmoid(self.fc2(h))\n",
        "        return h\n",
        "\n",
        "class MLP_2HL_Dropout_G(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(MLP_2HL_Dropout_G, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.attSize + opt.nz, opt.ngh)\n",
        "        self.fc2 = nn.Linear(opt.ngh, opt.ngh)\n",
        "        self.fc3 = nn.Linear(opt.ngh, opt.resSize)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        #self.prelu = nn.PReLU()\n",
        "        self.relu = nn.ReLU(True)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, noise, att):\n",
        "        h = torch.cat((noise, att), 1)\n",
        "        h = self.dropout(self.lrelu(self.fc1(h)))\n",
        "        h = self.dropout(self.lrelu(self.fc2(h)))\n",
        "        h = self.relu(self.fc3(h))\n",
        "        return h\n",
        "\n",
        "class MLP_3HL_G(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(MLP_3HL_G, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.attSize + opt.nz, opt.ngh)\n",
        "        self.fc2 = nn.Linear(opt.ngh, opt.ngh)\n",
        "        self.fc3 = nn.Linear(opt.ngh, opt.ngh)\n",
        "        self.fc4 = nn.Linear(opt.ngh, opt.resSize)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        #self.prelu = nn.PReLU()\n",
        "        self.relu = nn.ReLU(True)\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, noise, att):\n",
        "        h = torch.cat((noise, att), 1)\n",
        "        h = self.lrelu(self.fc1(h))\n",
        "        h = self.lrelu(self.fc2(h))\n",
        "        h = self.lrelu(self.fc3(h))\n",
        "        h = self.relu(self.fc4(h))\n",
        "        return h\n",
        "\n",
        "class MLP_2HL_G(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(MLP_2HL_G, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.attSize + opt.nz, opt.ngh)\n",
        "        self.fc2 = nn.Linear(opt.ngh, opt.ngh)\n",
        "        self.fc3 = nn.Linear(opt.ngh, opt.resSize)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        #self.prelu = nn.PReLU()\n",
        "        self.relu = nn.ReLU(True)\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, noise, att):\n",
        "        h = torch.cat((noise, att), 1)\n",
        "        h = self.lrelu(self.fc1(h))\n",
        "        h = self.lrelu(self.fc2(h))\n",
        "        h = self.relu(self.fc3(h))\n",
        "        return h\n",
        "\n",
        "class MLP_Dropout_G(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(MLP_Dropout_G, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.attSize + opt.nz, opt.ngh)\n",
        "        self.fc2 = nn.Linear(opt.ngh, opt.resSize)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        self.relu = nn.ReLU(True)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, noise, att):\n",
        "        h = torch.cat((noise, att), 1)\n",
        "        h = self.dropout(self.lrelu(self.fc1(h)))\n",
        "        h = self.relu(self.fc2(h))\n",
        "        return h\n",
        "\n",
        "class MLP_G(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(MLP_G, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.attSize + opt.nz, opt.ngh)\n",
        "        self.fc2 = nn.Linear(opt.ngh, opt.resSize)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        #self.prelu = nn.PReLU()\n",
        "        self.relu = nn.ReLU(True)\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, noise, att):\n",
        "        h = torch.cat((noise, att), 1)\n",
        "        h = self.lrelu(self.fc1(h))\n",
        "        h = self.relu(self.fc2(h))\n",
        "        return h\n",
        "\n",
        "class MLP_2048_1024_Dropout_G(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(MLP_2048_1024_Dropout_G, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.attSize + opt.nz, opt.ngh)\n",
        "        #self.fc2 = nn.Linear(opt.ngh, opt.ngh)\n",
        "        self.fc2 = nn.Linear(opt.ngh, 1024)\n",
        "        self.fc3 = nn.Linear(1024, opt.resSize)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        #self.prelu = nn.PReLU()\n",
        "        #self.relu = nn.ReLU(True)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, noise, att):\n",
        "        h = torch.cat((noise, att), 1)\n",
        "        h = self.dropout(self.lrelu(self.fc1(h)))\n",
        "        h = self.dropout(self.lrelu(self.fc2(h)))\n",
        "        h = self.fc3(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class MLP_SKIP_G(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super(MLP_SKIP_G, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.attSize + opt.nz, opt.ngh)\n",
        "        #self.fc2 = nn.Linear(opt.ngh, opt.ngh)\n",
        "        #self.fc2 = nn.Linear(opt.ngh, 1024)\n",
        "        self.fc2 = nn.Linear(opt.ngh, opt.resSize)\n",
        "        self.fc_skip = nn.Linear(opt.attSize, opt.resSize)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        #self.prelu = nn.PReLU()\n",
        "        self.relu = nn.ReLU(True)\n",
        "        \n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, noise, att):\n",
        "        h = torch.cat((noise, att), 1)\n",
        "        h = self.lrelu(self.fc1(h))\n",
        "        #h = self.lrelu(self.fc2(h))\n",
        "        h = self.relu(self.fc2(h))\n",
        "        h2 = self.fc_skip(att)\n",
        "        return h+h2\n",
        "\n",
        "\n",
        "#util\n",
        "class MLP_SKIP_D(nn.Module):\n",
        "    def __init__(self, opt): \n",
        "        super(MLP_SKIP_D, self).__init__()\n",
        "        self.fc1 = nn.Linear(opt.resSize + opt.attSize, opt.ndh)\n",
        "        self.fc2 = nn.Linear(opt.ndh, 1)\n",
        "        self.fc_skip = nn.Linear(opt.attSize, opt.ndh)\n",
        "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.apply(weights_init)\n",
        "    \n",
        "    def forward(self, x, att):\n",
        "        h = torch.cat((x, att), 1) \n",
        "        h = self.lrelu(self.fc1(h))\n",
        "        h2 = self.lrelu(self.fc_skip(att))\n",
        "        h = self.sigmoid(self.fc2(h+h2))\n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yEQ9H9inBnU",
        "colab_type": "text"
      },
      "source": [
        "# lisgan.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZc02z3HnKFd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8bdef7e3-04d1-4b23-ed39-22dc2202c431"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "import sys\n",
        "import numpy as np\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class parser:\n",
        "  pass\n",
        "parser.dataset='AWA2'\n",
        "parser.dataroot='/content/drive/My Drive/Colab Notebooks/Project/xlsa17/data'\n",
        "parser.matdataset=True\n",
        "parser.image_embedding='res101'\n",
        "parser.class_embedding='att'\n",
        "parser.syn_num=100\n",
        "parser.gzsl=False\n",
        "parser.preprocessing=False,\n",
        "parser.standardization=False\n",
        "parser.validation=False\n",
        "parser.workers=2\n",
        "parser.batch_size=64\n",
        "parser.resSize=2048\n",
        "parser.attSize=85 #1024\n",
        "parser.nz= 85 #312\n",
        "parser.ngh=4096\n",
        "parser.ndh=1024\n",
        "parser.nepoch=2000\n",
        "parser.critic_iter=5\n",
        "parser.lambda1=10\n",
        "parser.cls_weight=1\n",
        "parser.lr=0.0001\n",
        "parser.classifier_lr=0.001\n",
        "parser.beta1=0.5\n",
        "parser.cuda=False\n",
        "parser.ngpu=1\n",
        "parser.pretrain_classifier=''\n",
        "parser.netG=''\n",
        "parser.netD=''\n",
        "parser.netG_name=''\n",
        "parser.netD_name=''\n",
        "parser.outf='./checkpoint/'\n",
        "#parser.('--outname', help='folder to output data and model checkpoints')\n",
        "parser.save_every=100\n",
        "parser.print_every=1\n",
        "parser.val_every=10\n",
        "parser.start_epoch=0\n",
        "#parser.manualSeed', type=int, help='manual seed')\n",
        "parser.nclass_all=50 #200\n",
        "parser.ratio=0.2\n",
        "parser.proto_param1=0.01\n",
        "parser.proto_param2=0.01\n",
        "parser.loss_syn_num=20\n",
        "parser.n_clusters=3\n",
        "\n",
        "\n",
        "def GetNowTime():\n",
        "    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time()))\n",
        "\n",
        "print(GetNowTime())\n",
        "print('Begin run!!!')\n",
        "since = time.time()\n",
        "\n",
        "opt = parser()\n",
        "print('Params: dataset={:s}, GZSL={:s}, ratio={:.1f}, cls_weight={:.4f}, proto_param1={:.4f}, proto_param2={:.4f}'.format(\n",
        "    opt.dataset, str(opt.gzsl), opt.ratio, opt.cls_weight,opt.proto_param1, opt.proto_param2))\n",
        "sys.stdout.flush()\n",
        "\n",
        "'''\n",
        "if opt.manualSeed is None:\n",
        "    opt.manualSeed = random.randint(1, 10000)\n",
        "print(\"Random Seed: \", opt.manualSeed)\n",
        "random.seed(opt.manualSeed)\n",
        "torch.manual_seed(opt.manualSeed)\n",
        "if opt.cuda:\n",
        "    torch.cuda.manual_seed_all(opt.manualSeed)\n",
        "'''\n",
        "cudnn.benchmark = True\n",
        "\n",
        "if torch.cuda.is_available() and not opt.cuda:\n",
        "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "# load data\n",
        "data = DATA_LOADER(opt)\n",
        "print(\"Training samples: \", data.ntrain)\n",
        "#util\n",
        "# initialize generator and discriminator\n",
        "netG = MLP_G(opt)\n",
        "if opt.netG != '':\n",
        "    netG.load_state_dict(torch.load(opt.netG))\n",
        "# print(netG)\n",
        "#classifier\n",
        "netD = MLP_CRITIC(opt)\n",
        "if opt.netD != '':\n",
        "    netD.load_state_dict(torch.load(opt.netD))\n",
        "# print(netD)\n",
        "\n",
        "# classification loss, Equation (4) of the paper\n",
        "cls_criterion = nn.NLLLoss()\n",
        "\n",
        "input_res = torch.FloatTensor(opt.batch_size, opt.resSize)\n",
        "input_att = torch.FloatTensor(opt.batch_size, opt.attSize)\n",
        "noise = torch.FloatTensor(opt.batch_size, opt.nz)\n",
        "one = torch.FloatTensor([1])\n",
        "mone = one * -1\n",
        "input_label = torch.LongTensor(opt.batch_size)\n",
        "\n",
        "if torch.cuda:\n",
        "    netD.cuda()\n",
        "    netG.cuda()\n",
        "    input_res = input_res.cuda()\n",
        "    noise, input_att = noise.cuda(), input_att.cuda()\n",
        "    one = one.cuda()\n",
        "    mone = mone.cuda()\n",
        "    cls_criterion.cuda()\n",
        "    input_label = input_label.cuda()\n",
        "\n",
        "#classifier\n",
        "def sample():\n",
        "    batch_feature, batch_label, batch_att = data.next_batch(opt.batch_size)\n",
        "    input_res.copy_(batch_feature)\n",
        "    input_att.copy_(batch_att)\n",
        "    input_label.copy_(map_label(batch_label, data.seenclasses))\n",
        "\n",
        "\n",
        "def generate_syn_feature(netG, classes, attribute, num):\n",
        "    nclass = classes.size(0)\n",
        "    syn_feature = torch.FloatTensor(nclass * num, opt.resSize)\n",
        "    syn_label = torch.LongTensor(nclass * num)\n",
        "    syn_att = torch.FloatTensor(num, opt.attSize)\n",
        "    syn_noise = torch.FloatTensor(num, opt.nz)\n",
        "    if torch.cuda:\n",
        "        syn_att = syn_att.cuda()\n",
        "        syn_noise = syn_noise.cuda()\n",
        "\n",
        "    for i in range(nclass):\n",
        "        iclass = classes[i]\n",
        "        iclass_att = attribute[iclass]\n",
        "        syn_att.copy_(iclass_att.repeat(num, 1))\n",
        "        syn_noise.normal_(0, 1)\n",
        "        output = netG(Variable(syn_noise, volatile=True), Variable(syn_att, volatile=True))\n",
        "        syn_feature.narrow(0, i * num, num).copy_(output.data.cpu())\n",
        "        syn_label.narrow(0, i * num, num).fill_(iclass)\n",
        "\n",
        "    return syn_feature, syn_label\n",
        "\n",
        "def generate_syn_feature_with_grad(netG, classes, attribute, num):\n",
        "    nclass = classes.size(0)\n",
        "    # syn_feature = torch.FloatTensor(nclass*num, opt.resSize)\n",
        "    syn_label = torch.LongTensor(nclass * num)\n",
        "    syn_att = torch.FloatTensor(nclass * num, opt.attSize)\n",
        "    syn_noise = torch.FloatTensor(nclass * num, opt.nz)\n",
        "    if torch.cuda:\n",
        "        syn_att = syn_att.cuda()\n",
        "        syn_noise = syn_noise.cuda()\n",
        "        syn_label = syn_label.cuda()\n",
        "    syn_noise.normal_(0, 1)\n",
        "    for i in range(nclass):\n",
        "        iclass = classes[i]\n",
        "        iclass_att = attribute[iclass]\n",
        "        syn_att.narrow(0, i * num, num).copy_(iclass_att.repeat(num, 1))\n",
        "        syn_label.narrow(0, i * num, num).fill_(iclass)\n",
        "    syn_feature = netG(Variable(syn_noise), Variable(syn_att))\n",
        "    return syn_feature, syn_label.cpu()\n",
        "\n",
        "def map_label(label, classes):\n",
        "    mapped_label = torch.LongTensor(label.size())\n",
        "    for i in range(classes.size(0)):\n",
        "        mapped_label[label==classes[i]] = i\n",
        "    return mapped_label\n",
        "#util\n",
        "\n",
        "def pairwise_distances(x, y=None):\n",
        "    '''\n",
        "    Input: x is a Nxd matrix\n",
        "           y is an optional Mxd matirx\n",
        "    Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]\n",
        "            if y is not given then use 'y=x'.\n",
        "    i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n",
        "    '''\n",
        "    x_norm = (x ** 2).sum(1).view(-1, 1)\n",
        "    if y is not None:\n",
        "        y_t = torch.transpose(y, 0, 1)\n",
        "        y_norm = (y ** 2).sum(1).view(1, -1)\n",
        "    else:\n",
        "        y_t = torch.transpose(x, 0, 1)\n",
        "        y_norm = x_norm.view(1, -1)\n",
        "\n",
        "    dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
        "    # Ensure diagonal is zero if x=y\n",
        "    if y is None:\n",
        "        dist = dist - torch.diag(dist.diag)\n",
        "    return torch.clamp(dist, 0.0, np.inf)\n",
        "\n",
        "# setup optimizer\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
        "\n",
        "\n",
        "def calc_gradient_penalty(netD, real_data, fake_data, input_att):\n",
        "    # print real_data.size()\n",
        "    alpha = torch.rand(opt.batch_size, 1)\n",
        "    alpha = alpha.expand(real_data.size())\n",
        "    if torch.cuda:\n",
        "        alpha = alpha.cuda()\n",
        "\n",
        "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
        "\n",
        "    if torch.cuda:\n",
        "        interpolates = interpolates.cuda()\n",
        "\n",
        "    interpolates = Variable(interpolates, requires_grad=True)\n",
        "\n",
        "    disc_interpolates = netD(interpolates, Variable(input_att))\n",
        "\n",
        "    ones = torch.ones(disc_interpolates.size())\n",
        "    if torch.cuda:\n",
        "        ones = ones.cuda()\n",
        "\n",
        "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
        "                              grad_outputs=ones,\n",
        "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * opt.lambda1\n",
        "    return gradient_penalty\n",
        "\n",
        "\n",
        "# train a classifier on seen classes, obtain \\theta of Equation (4)\n",
        "pretrain_cls = CLASSIFIER(data.train_feature, map_label(data.train_label, data.seenclasses),\n",
        "                                     data.seenclasses.size(0), opt.resSize, opt.cuda, 0.001, 0.5, 100, 100,\n",
        "                                     opt.pretrain_classifier)\n",
        "\n",
        "# freeze the classifier during the optimization\n",
        "'''\n",
        "for p in pretrain_cls.parameters():  # set requires_grad to False\n",
        "    p.requires_grad = False\n",
        "'''\n",
        "#classifier\n",
        "for epoch in range(opt.nepoch):\n",
        "    FP = 0\n",
        "    mean_lossD = 0\n",
        "    mean_lossG = 0\n",
        "\n",
        "    for i in range(0, data.ntrain, opt.batch_size):\n",
        "\n",
        "        for p in netD.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        for iter_d in range(opt.critic_iter):\n",
        "            sample()\n",
        "            netD.zero_grad()\n",
        "            sparse_real = opt.resSize - input_res[1].gt(0).sum()\n",
        "            input_resv = Variable(input_res)\n",
        "            input_attv = Variable(input_att)\n",
        "\n",
        "            criticD_real = netD(input_resv, input_attv)\n",
        "            criticD_real = criticD_real.mean()\n",
        "            criticD_real.backward(mone)\n",
        "\n",
        "            noise.normal_(0, 1)\n",
        "            noisev = Variable(noise)\n",
        "            fake = netG(noisev, input_attv)\n",
        "            fake_norm = fake.data[0].norm()\n",
        "            sparse_fake = fake.data[0].eq(0).sum()\n",
        "            criticD_fake = netD(fake.detach(), input_attv)\n",
        "            criticD_fake = criticD_fake.mean()\n",
        "            criticD_fake.backward(one)\n",
        "\n",
        "            gradient_penalty = calc_gradient_penalty(netD, input_res, fake.data, input_att)\n",
        "            gradient_penalty.backward()\n",
        "\n",
        "            Wasserstein_D = criticD_real - criticD_fake\n",
        "            D_cost = criticD_fake - criticD_real + gradient_penalty\n",
        "            optimizerD.step()\n",
        "\n",
        "        for p in netD.parameters():  # reset requires_grad\n",
        "            p.requires_grad = False  # avoid computation\n",
        "\n",
        "        netG.zero_grad()\n",
        "        input_attv = Variable(input_att)\n",
        "        noise.normal_(0, 1)\n",
        "        noisev = Variable(noise)\n",
        "        fake = netG(noisev, input_attv)\n",
        "        criticG_fake = netD(fake, input_attv)\n",
        "        criticG_fake = criticG_fake.mean()\n",
        "        G_cost = -criticG_fake\n",
        "        # classification loss\n",
        "        #print(pretrain_cls, \"pretrain_cls\")\n",
        "        #print(input_label, \"input_label\")\n",
        "        #print(Variable(input_label), \"Variable(input_label)\")\n",
        "        c_errG = cls_criterion(fake, Variable(input_label))\n",
        "\n",
        "        labels = Variable(input_label.view(opt.batch_size, 1))\n",
        "        real_proto = Variable(data.real_proto.cuda())\n",
        "        dists1 = pairwise_distances(fake,real_proto)\n",
        "        min_idx1 = torch.zeros(opt.batch_size, data.train_cls_num)\n",
        "        for i in range(data.train_cls_num):\n",
        "            min_idx1[:,i] = torch.min(dists1.data[:,i*opt.n_clusters:(i+1)*opt.n_clusters],dim=1)[1] + i*opt.n_clusters\n",
        "        min_idx1 = Variable(min_idx1.long().cuda())\n",
        "        loss2 = dists1.gather(1,min_idx1).gather(1,labels).squeeze().view(-1).mean()\n",
        "\n",
        "        seen_feature, seen_label = generate_syn_feature_with_grad(netG, data.seenclasses, data.attribute,opt.loss_syn_num)\n",
        "        seen_mapped_label = map_label(seen_label, data.seenclasses)\n",
        "        transform_matrix = torch.zeros(data.train_cls_num, seen_feature.size(0))  # 150x7057\n",
        "        for i in range(data.train_cls_num):\n",
        "            sample_idx = (seen_mapped_label == i).nonzero().squeeze()\n",
        "            if sample_idx.numel() == 0:\n",
        "                continue\n",
        "            else:\n",
        "                cls_fea_num = sample_idx.numel()\n",
        "                transform_matrix[i][sample_idx] = 1 / cls_fea_num * torch.ones(1, cls_fea_num).squeeze()\n",
        "        transform_matrix = Variable(transform_matrix.cuda())\n",
        "        fake_proto = torch.mm(transform_matrix, seen_feature)  # 150x2048\n",
        "        dists2 = pairwise_distances(fake_proto,Variable(data.real_proto.cuda())) # 150 x 450\n",
        "        min_idx2 = torch.zeros(data.train_cls_num, data.train_cls_num)\n",
        "        for i in range(data.train_cls_num):\n",
        "            min_idx2[:,i] = torch.min(dists2.data[:,i*opt.n_clusters:(i+1)*opt.n_clusters],dim=1)[1] + i*opt.n_clusters\n",
        "        min_idx2 = Variable(min_idx2.long().cuda())\n",
        "        lbl_idx = Variable(torch.LongTensor(list(range(data.train_cls_num))).cuda())\n",
        "        loss1 = dists2.gather(1,min_idx2).gather(1,lbl_idx.unsqueeze(1)).squeeze().mean()\n",
        "\n",
        "        errG = G_cost + opt.cls_weight * c_errG + opt.proto_param2 * loss2 + opt.proto_param1 * loss1\n",
        "        errG.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "    print('EP[%d/%d]************************************************************************************' % (\n",
        "    epoch, opt.nepoch))\n",
        "\n",
        "    # evaluate the model, set G to evaluation mode\n",
        "    netG.eval()\n",
        "    # Generalized zero-shot learning\n",
        "    if opt.gzsl:\n",
        "        syn_feature, syn_label = generate_syn_feature(netG, data.unseenclasses, data.attribute, opt.syn_num)\n",
        "        train_X = torch.cat((data.train_feature, syn_feature), 0)\n",
        "        train_Y = torch.cat((data.train_label, syn_label), 0)\n",
        "        nclass = opt.nclass_all\n",
        "        cls = CLASSIFIER2(train_X, train_Y, data, nclass, opt.cuda, opt.classifier_lr, 0.5, 50, 2*opt.syn_num,True)\n",
        "        # print('unseen=%.4f, seen=%.4f, h=%.4f' % (cls.acc_unseen, cls.acc_seen, cls.H))\n",
        "    # Zero-shot learning\n",
        "    else:\n",
        "        syn_feature, syn_label = generate_syn_feature(netG, data.unseenclasses, data.attribute, opt.syn_num)\n",
        "        cls = CLASSIFIER2(syn_feature, map_label(syn_label, data.unseenclasses), data,\n",
        "                                     data.unseenclasses.size(0), opt.cuda, opt.classifier_lr, 0.5, 50, 2*opt.syn_num,\n",
        "                                     False, opt.ratio, epoch)\n",
        "        # acc = cls.acc util\n",
        "        # print('unseen class accuracy= ', cls.acc)\n",
        "    del cls\n",
        "    cls = None\n",
        "    # reset G to training mode\n",
        "    netG.train()\n",
        "    sys.stdout.flush()\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('End run!!!')\n",
        "print('Time Elapsed: {}'.format(time_elapsed))\n",
        "print(GetNowTime())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-18 18:35:49\n",
            "Begin run!!!\n",
            "Params: dataset=AWA2, GZSL=False, ratio=0.2, cls_weight=1.0000, proto_param1=0.0100, proto_param2=0.0100\n",
            "WARNING: You have a CUDA device, so you should probably run with --cuda\n",
            "Training samples:  23527\n",
            "EP[0/2000]************************************************************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:150: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:353: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "First Acc: 26.88%\n",
            "1NN   Acc: 28.11%\n",
            "FC    Acc: 23.64%\n",
            "EP[1/2000]************************************************************************************\n",
            "First Acc: 48.91%\n",
            "1NN   Acc: 40.25%\n",
            "FC    Acc: 46.26%\n",
            "EP[2/2000]************************************************************************************\n",
            "First Acc: 63.75%\n",
            "1NN   Acc: 53.76%\n",
            "FC    Acc: 65.15%\n",
            "EP[3/2000]************************************************************************************\n",
            "First Acc: 68.02%\n",
            "1NN   Acc: 63.80%\n",
            "FC    Acc: 68.55%\n",
            "EP[4/2000]************************************************************************************\n",
            "First Acc: 64.93%\n",
            "1NN   Acc: 59.23%\n",
            "FC    Acc: 65.03%\n",
            "EP[5/2000]************************************************************************************\n",
            "First Acc: 64.81%\n",
            "1NN   Acc: 59.42%\n",
            "FC    Acc: 64.57%\n",
            "EP[6/2000]************************************************************************************\n",
            "First Acc: 64.50%\n",
            "1NN   Acc: 59.89%\n",
            "FC    Acc: 65.98%\n",
            "EP[7/2000]************************************************************************************\n",
            "First Acc: 66.81%\n",
            "1NN   Acc: 63.67%\n",
            "FC    Acc: 66.90%\n",
            "EP[8/2000]************************************************************************************\n",
            "First Acc: 64.76%\n",
            "1NN   Acc: 62.44%\n",
            "FC    Acc: 66.65%\n",
            "EP[9/2000]************************************************************************************\n",
            "First Acc: 65.47%\n",
            "1NN   Acc: 62.31%\n",
            "FC    Acc: 65.02%\n",
            "EP[10/2000]************************************************************************************\n",
            "First Acc: 63.27%\n",
            "1NN   Acc: 62.03%\n",
            "FC    Acc: 64.04%\n",
            "EP[11/2000]************************************************************************************\n",
            "First Acc: 63.34%\n",
            "1NN   Acc: 63.04%\n",
            "FC    Acc: 65.46%\n",
            "EP[12/2000]************************************************************************************\n",
            "First Acc: 64.52%\n",
            "1NN   Acc: 62.36%\n",
            "FC    Acc: 65.31%\n",
            "EP[13/2000]************************************************************************************\n",
            "First Acc: 64.13%\n",
            "1NN   Acc: 63.83%\n",
            "FC    Acc: 65.63%\n",
            "EP[14/2000]************************************************************************************\n",
            "First Acc: 63.62%\n",
            "1NN   Acc: 61.90%\n",
            "FC    Acc: 63.11%\n",
            "EP[15/2000]************************************************************************************\n",
            "First Acc: 62.96%\n",
            "1NN   Acc: 60.50%\n",
            "FC    Acc: 63.25%\n",
            "EP[16/2000]************************************************************************************\n",
            "First Acc: 59.98%\n",
            "1NN   Acc: 56.24%\n",
            "FC    Acc: 59.78%\n",
            "EP[17/2000]************************************************************************************\n",
            "First Acc: 61.04%\n",
            "1NN   Acc: 58.27%\n",
            "FC    Acc: 61.00%\n",
            "EP[18/2000]************************************************************************************\n",
            "First Acc: 63.40%\n",
            "1NN   Acc: 60.21%\n",
            "FC    Acc: 62.61%\n",
            "EP[19/2000]************************************************************************************\n",
            "First Acc: 62.82%\n",
            "1NN   Acc: 58.93%\n",
            "FC    Acc: 63.60%\n",
            "EP[20/2000]************************************************************************************\n",
            "First Acc: 62.64%\n",
            "1NN   Acc: 61.41%\n",
            "FC    Acc: 62.16%\n",
            "EP[21/2000]************************************************************************************\n",
            "First Acc: 61.98%\n",
            "1NN   Acc: 62.39%\n",
            "FC    Acc: 63.56%\n",
            "EP[22/2000]************************************************************************************\n",
            "First Acc: 59.75%\n",
            "1NN   Acc: 59.87%\n",
            "FC    Acc: 60.78%\n",
            "EP[23/2000]************************************************************************************\n",
            "First Acc: 57.22%\n",
            "1NN   Acc: 55.97%\n",
            "FC    Acc: 57.11%\n",
            "EP[24/2000]************************************************************************************\n",
            "First Acc: 59.01%\n",
            "1NN   Acc: 59.32%\n",
            "FC    Acc: 58.76%\n",
            "EP[25/2000]************************************************************************************\n",
            "First Acc: 62.59%\n",
            "1NN   Acc: 62.50%\n",
            "FC    Acc: 64.07%\n",
            "EP[26/2000]************************************************************************************\n",
            "First Acc: 60.37%\n",
            "1NN   Acc: 55.43%\n",
            "FC    Acc: 59.38%\n",
            "EP[27/2000]************************************************************************************\n",
            "First Acc: 62.10%\n",
            "1NN   Acc: 60.24%\n",
            "FC    Acc: 63.75%\n",
            "EP[28/2000]************************************************************************************\n",
            "First Acc: 58.56%\n",
            "1NN   Acc: 57.27%\n",
            "FC    Acc: 59.28%\n",
            "EP[29/2000]************************************************************************************\n",
            "First Acc: 62.48%\n",
            "1NN   Acc: 62.08%\n",
            "FC    Acc: 63.36%\n",
            "EP[30/2000]************************************************************************************\n",
            "First Acc: 60.26%\n",
            "1NN   Acc: 58.04%\n",
            "FC    Acc: 62.96%\n",
            "EP[31/2000]************************************************************************************\n",
            "First Acc: 61.62%\n",
            "1NN   Acc: 61.48%\n",
            "FC    Acc: 64.83%\n",
            "EP[32/2000]************************************************************************************\n",
            "First Acc: 59.59%\n",
            "1NN   Acc: 60.14%\n",
            "FC    Acc: 60.42%\n",
            "EP[33/2000]************************************************************************************\n",
            "First Acc: 59.66%\n",
            "1NN   Acc: 60.37%\n",
            "FC    Acc: 59.84%\n",
            "EP[34/2000]************************************************************************************\n",
            "First Acc: 60.35%\n",
            "1NN   Acc: 58.63%\n",
            "FC    Acc: 59.88%\n",
            "EP[35/2000]************************************************************************************\n",
            "First Acc: 61.38%\n",
            "1NN   Acc: 61.42%\n",
            "FC    Acc: 61.27%\n",
            "EP[36/2000]************************************************************************************\n",
            "First Acc: 63.72%\n",
            "1NN   Acc: 64.92%\n",
            "FC    Acc: 65.17%\n",
            "EP[37/2000]************************************************************************************\n",
            "First Acc: 60.47%\n",
            "1NN   Acc: 60.85%\n",
            "FC    Acc: 62.03%\n",
            "EP[38/2000]************************************************************************************\n",
            "First Acc: 58.17%\n",
            "1NN   Acc: 57.80%\n",
            "FC    Acc: 58.25%\n",
            "EP[39/2000]************************************************************************************\n",
            "First Acc: 59.80%\n",
            "1NN   Acc: 57.85%\n",
            "FC    Acc: 59.84%\n",
            "EP[40/2000]************************************************************************************\n",
            "First Acc: 61.71%\n",
            "1NN   Acc: 61.07%\n",
            "FC    Acc: 61.03%\n",
            "EP[41/2000]************************************************************************************\n",
            "First Acc: 61.82%\n",
            "1NN   Acc: 62.97%\n",
            "FC    Acc: 63.24%\n",
            "EP[42/2000]************************************************************************************\n",
            "First Acc: 62.15%\n",
            "1NN   Acc: 61.98%\n",
            "FC    Acc: 64.29%\n",
            "EP[43/2000]************************************************************************************\n",
            "First Acc: 62.08%\n",
            "1NN   Acc: 60.70%\n",
            "FC    Acc: 61.96%\n",
            "EP[44/2000]************************************************************************************\n",
            "First Acc: 59.91%\n",
            "1NN   Acc: 59.59%\n",
            "FC    Acc: 59.97%\n",
            "EP[45/2000]************************************************************************************\n",
            "First Acc: 58.19%\n",
            "1NN   Acc: 59.01%\n",
            "FC    Acc: 60.03%\n",
            "EP[46/2000]************************************************************************************\n",
            "First Acc: 61.75%\n",
            "1NN   Acc: 60.78%\n",
            "FC    Acc: 62.92%\n",
            "EP[47/2000]************************************************************************************\n",
            "First Acc: 59.47%\n",
            "1NN   Acc: 59.58%\n",
            "FC    Acc: 60.51%\n",
            "EP[48/2000]************************************************************************************\n",
            "First Acc: 57.93%\n",
            "1NN   Acc: 56.68%\n",
            "FC    Acc: 59.37%\n",
            "EP[49/2000]************************************************************************************\n",
            "First Acc: 61.35%\n",
            "1NN   Acc: 59.52%\n",
            "FC    Acc: 61.79%\n",
            "EP[50/2000]************************************************************************************\n",
            "First Acc: 60.64%\n",
            "1NN   Acc: 60.31%\n",
            "FC    Acc: 61.20%\n",
            "EP[51/2000]************************************************************************************\n",
            "First Acc: 62.33%\n",
            "1NN   Acc: 61.48%\n",
            "FC    Acc: 63.90%\n",
            "EP[52/2000]************************************************************************************\n",
            "First Acc: 61.14%\n",
            "1NN   Acc: 59.75%\n",
            "FC    Acc: 61.79%\n",
            "EP[53/2000]************************************************************************************\n",
            "First Acc: 59.02%\n",
            "1NN   Acc: 56.72%\n",
            "FC    Acc: 58.70%\n",
            "EP[54/2000]************************************************************************************\n",
            "First Acc: 59.76%\n",
            "1NN   Acc: 57.80%\n",
            "FC    Acc: 61.31%\n",
            "EP[55/2000]************************************************************************************\n",
            "First Acc: 61.80%\n",
            "1NN   Acc: 61.16%\n",
            "FC    Acc: 62.62%\n",
            "EP[56/2000]************************************************************************************\n",
            "First Acc: 61.89%\n",
            "1NN   Acc: 60.60%\n",
            "FC    Acc: 64.16%\n",
            "EP[57/2000]************************************************************************************\n",
            "First Acc: 59.14%\n",
            "1NN   Acc: 56.71%\n",
            "FC    Acc: 58.96%\n",
            "EP[58/2000]************************************************************************************\n",
            "First Acc: 61.17%\n",
            "1NN   Acc: 61.82%\n",
            "FC    Acc: 62.27%\n",
            "EP[59/2000]************************************************************************************\n",
            "First Acc: 58.74%\n",
            "1NN   Acc: 57.44%\n",
            "FC    Acc: 59.66%\n",
            "EP[60/2000]************************************************************************************\n",
            "First Acc: 58.18%\n",
            "1NN   Acc: 55.57%\n",
            "FC    Acc: 58.97%\n",
            "EP[61/2000]************************************************************************************\n",
            "First Acc: 63.48%\n",
            "1NN   Acc: 60.80%\n",
            "FC    Acc: 65.18%\n",
            "EP[62/2000]************************************************************************************\n",
            "First Acc: 60.31%\n",
            "1NN   Acc: 61.14%\n",
            "FC    Acc: 60.00%\n",
            "EP[63/2000]************************************************************************************\n",
            "First Acc: 59.53%\n",
            "1NN   Acc: 59.78%\n",
            "FC    Acc: 59.94%\n",
            "EP[64/2000]************************************************************************************\n",
            "First Acc: 61.48%\n",
            "1NN   Acc: 59.36%\n",
            "FC    Acc: 61.27%\n",
            "EP[65/2000]************************************************************************************\n",
            "First Acc: 59.83%\n",
            "1NN   Acc: 59.49%\n",
            "FC    Acc: 61.77%\n",
            "EP[66/2000]************************************************************************************\n",
            "First Acc: 57.74%\n",
            "1NN   Acc: 55.58%\n",
            "FC    Acc: 58.39%\n",
            "EP[67/2000]************************************************************************************\n",
            "First Acc: 58.13%\n",
            "1NN   Acc: 58.43%\n",
            "FC    Acc: 58.33%\n",
            "EP[68/2000]************************************************************************************\n",
            "First Acc: 58.99%\n",
            "1NN   Acc: 57.48%\n",
            "FC    Acc: 59.33%\n",
            "EP[69/2000]************************************************************************************\n",
            "First Acc: 59.84%\n",
            "1NN   Acc: 59.02%\n",
            "FC    Acc: 60.05%\n",
            "EP[70/2000]************************************************************************************\n",
            "First Acc: 60.87%\n",
            "1NN   Acc: 56.66%\n",
            "FC    Acc: 61.12%\n",
            "EP[71/2000]************************************************************************************\n",
            "First Acc: 61.17%\n",
            "1NN   Acc: 58.55%\n",
            "FC    Acc: 61.78%\n",
            "EP[72/2000]************************************************************************************\n",
            "First Acc: 61.92%\n",
            "1NN   Acc: 61.52%\n",
            "FC    Acc: 63.25%\n",
            "EP[73/2000]************************************************************************************\n",
            "First Acc: 59.08%\n",
            "1NN   Acc: 57.77%\n",
            "FC    Acc: 58.94%\n",
            "EP[74/2000]************************************************************************************\n",
            "First Acc: 62.01%\n",
            "1NN   Acc: 59.99%\n",
            "FC    Acc: 61.50%\n",
            "EP[75/2000]************************************************************************************\n",
            "First Acc: 60.04%\n",
            "1NN   Acc: 58.80%\n",
            "FC    Acc: 60.35%\n",
            "EP[76/2000]************************************************************************************\n",
            "First Acc: 62.31%\n",
            "1NN   Acc: 58.17%\n",
            "FC    Acc: 61.70%\n",
            "EP[77/2000]************************************************************************************\n",
            "First Acc: 59.88%\n",
            "1NN   Acc: 60.56%\n",
            "FC    Acc: 59.06%\n",
            "EP[78/2000]************************************************************************************\n",
            "First Acc: 59.91%\n",
            "1NN   Acc: 59.12%\n",
            "FC    Acc: 60.49%\n",
            "EP[79/2000]************************************************************************************\n",
            "First Acc: 58.90%\n",
            "1NN   Acc: 57.09%\n",
            "FC    Acc: 59.24%\n",
            "EP[80/2000]************************************************************************************\n",
            "First Acc: 64.02%\n",
            "1NN   Acc: 61.10%\n",
            "FC    Acc: 64.01%\n",
            "EP[81/2000]************************************************************************************\n",
            "First Acc: 60.49%\n",
            "1NN   Acc: 59.36%\n",
            "FC    Acc: 60.76%\n",
            "EP[82/2000]************************************************************************************\n",
            "First Acc: 57.91%\n",
            "1NN   Acc: 57.27%\n",
            "FC    Acc: 58.27%\n",
            "EP[83/2000]************************************************************************************\n",
            "First Acc: 61.71%\n",
            "1NN   Acc: 60.41%\n",
            "FC    Acc: 63.48%\n",
            "EP[84/2000]************************************************************************************\n",
            "First Acc: 61.50%\n",
            "1NN   Acc: 60.12%\n",
            "FC    Acc: 62.94%\n",
            "EP[85/2000]************************************************************************************\n",
            "First Acc: 61.36%\n",
            "1NN   Acc: 59.42%\n",
            "FC    Acc: 61.92%\n",
            "EP[86/2000]************************************************************************************\n",
            "First Acc: 60.25%\n",
            "1NN   Acc: 61.48%\n",
            "FC    Acc: 61.33%\n",
            "EP[87/2000]************************************************************************************\n",
            "First Acc: 60.31%\n",
            "1NN   Acc: 60.89%\n",
            "FC    Acc: 60.74%\n",
            "EP[88/2000]************************************************************************************\n",
            "First Acc: 61.71%\n",
            "1NN   Acc: 58.93%\n",
            "FC    Acc: 60.87%\n",
            "EP[89/2000]************************************************************************************\n",
            "First Acc: 59.07%\n",
            "1NN   Acc: 57.79%\n",
            "FC    Acc: 59.93%\n",
            "EP[90/2000]************************************************************************************\n",
            "First Acc: 58.77%\n",
            "1NN   Acc: 56.63%\n",
            "FC    Acc: 57.58%\n",
            "EP[91/2000]************************************************************************************\n",
            "First Acc: 59.30%\n",
            "1NN   Acc: 56.46%\n",
            "FC    Acc: 59.42%\n",
            "EP[92/2000]************************************************************************************\n",
            "First Acc: 59.47%\n",
            "1NN   Acc: 59.74%\n",
            "FC    Acc: 59.71%\n",
            "EP[93/2000]************************************************************************************\n",
            "First Acc: 64.79%\n",
            "1NN   Acc: 62.37%\n",
            "FC    Acc: 65.19%\n",
            "EP[94/2000]************************************************************************************\n",
            "First Acc: 62.22%\n",
            "1NN   Acc: 61.31%\n",
            "FC    Acc: 63.06%\n",
            "EP[95/2000]************************************************************************************\n",
            "First Acc: 64.09%\n",
            "1NN   Acc: 64.62%\n",
            "FC    Acc: 63.80%\n",
            "EP[96/2000]************************************************************************************\n",
            "First Acc: 60.90%\n",
            "1NN   Acc: 59.54%\n",
            "FC    Acc: 62.52%\n",
            "EP[97/2000]************************************************************************************\n",
            "First Acc: 59.75%\n",
            "1NN   Acc: 58.65%\n",
            "FC    Acc: 60.61%\n",
            "EP[98/2000]************************************************************************************\n",
            "First Acc: 62.37%\n",
            "1NN   Acc: 61.76%\n",
            "FC    Acc: 62.69%\n",
            "EP[99/2000]************************************************************************************\n",
            "First Acc: 62.72%\n",
            "1NN   Acc: 63.63%\n",
            "FC    Acc: 63.66%\n",
            "EP[100/2000]************************************************************************************\n",
            "First Acc: 62.42%\n",
            "1NN   Acc: 59.33%\n",
            "FC    Acc: 60.78%\n",
            "EP[101/2000]************************************************************************************\n",
            "First Acc: 61.42%\n",
            "1NN   Acc: 62.43%\n",
            "FC    Acc: 62.84%\n",
            "EP[102/2000]************************************************************************************\n",
            "First Acc: 60.71%\n",
            "1NN   Acc: 58.21%\n",
            "FC    Acc: 60.23%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}